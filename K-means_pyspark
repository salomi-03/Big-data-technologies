from pyspark.sql import SparkSession
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
from pyspark.sql import Row

# Initialize Spark session
spark = SparkSession.builder.appName("KMeansExample").getOrCreate()

# Create a mock dataset
data = [
    Row(feature1=1.0, feature2=2.0),
    Row(feature1=1.5, feature2=1.8),
    Row(feature1=5.0, feature2=8.0),
    Row(feature1=8.0, feature2=8.0),
    Row(feature1=1.0, feature2=0.6),
    Row(feature1=9.0, feature2=11.0),
    Row(feature1=8.0, feature2=9.0),
    Row(feature1=7.0, feature2=6.0),
    Row(feature1=5.0, feature2=5.0),
    Row(feature1=6.0, feature2=4.0),
]

# Create DataFrame from the mock dataset
df = spark.createDataFrame(data)

# Show the original dataset
df.show()

# Step 1: Assemble features into a single vector column
assembler = VectorAssembler(inputCols=["feature1", "feature2"], outputCol="features")
assembled_df = assembler.transform(df)

# Step 2: Initialize the KMeans model
kmeans = KMeans(k=3, seed=1, featuresCol="features", predictionCol="prediction")

# Step 3: Train the KMeans model
kmeans_model = kmeans.fit(assembled_df)

# Step 4: Make predictions (assign clusters) for each point
predictions = kmeans_model.transform(assembled_df)

# Show the predictions (which cluster each point belongs to)
predictions.select("features", "prediction").show()

# Step 5: Evaluate the model (within-cluster sum of squared errors)
cost = kmeans_model.computeCost(assembled_df)
print(f"Within Set Sum of Squared Errors = {cost}")

# Step 6: Show the cluster centers
print("Cluster Centers: ")
for center in kmeans_model.clusterCenters():
    print(center)

# Stop the Spark session
spark.stop()
